<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>RIST</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Learning SO(3)-Invariant Semantic Correspondence via Local Shape Transform</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://chrockey.github.io/" target="_blank">Chunghyun Park</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="https://wookiekim.github.io/" target="_blank">Seungwook Kim</a><sup>1*</sup>,</span>
                  <span class="author-block">
                    <a href="https://jaesik.info/" target="_blank">Jaesik Park</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://cvlab.postech.ac.kr/~mcho/" target="_blank">Minsu Cho</a><sup>1</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>POSTECH, <sup>2</sup>Seoul National University<br>CVPR 2024</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                    <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2404.11156" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/chrockey/RIST" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser_ours.png" alt="Teaser Image" style="width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered">
        Our approach can predict SO(3)-invariant correspondences between rotated 3D shapes.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Establishing accurate 3D correspondences between shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised Rotation-Invariant 3D correspondence learner with Local Shape Transform, dubbed RIST, that learns to establish dense correspondences between shapes even under challenging intra-class variations and arbitrary orientations. Specifically, RIST learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shapes to be mapped to similar local shape descriptors, enabling RIST to establish dense point-wise correspondences. RIST demonstrates state-of-the-art performances on 3D part label transfer and semantic keypoint transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Overview of Proposed Method -->
<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="column is-centered has-text-centered">
      <h2 class="title is-3">RIST: 3D Rotation-Invariant Local Shape Transform</h2>
          <figure class="image">
            <img src="static/images/overview_cvpr_v3.png" alt="Overview of Proposed Method">
          </figure>
          <figcaption class="has-text-justified">The input point clouds are independently encoded to SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant point-wise local shape transforms. The local shape transforms map the global shape descriptor to local shape descriptors by infusing local semantics and geometry, which are used as inputs to the decoder for selfreconstruction. For cross-reconstruction, we apply the local shape transforms formulated from another point cloud to reconstruct the point cloud, ensuring that the local shape descriptors successfully capture generalizable local semantics and geometries. We supervise RIST via penalizing errors in self- and cross-reconstructions. At inference, we can leverage the local shape transforms for obtaining local shape descriptors, to identify the dense correspondences.</figcaption>
    </div>
  </div>
</section>
<!-- End overview of proposed method -->


<!-- Part Label Transfer -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <!-- Title -->
      <h2 class="title is-3">Part Label Transfer Results</h2>
          <figure class="image">
            <img src="static/images/part_label_transfer.png" alt="Overview of Proposed Method">
          </figure>
          <figcaption class="has-text-centered">Average IoU (%) of part label transfer on ShapeNetPart</figcaption>
      <!-- <h2 class="title is-3">Part Label Transfer</h2> -->
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/supp_qual_part_motorcycle.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Motorcycle
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/supp_qual_part_airplane.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Airplane
        </h2>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Keypoint Transfer -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <!-- Title -->
      <h2 class="title is-3">Keypoint Transfer Results</h2>
          <figure class="image">
            <img src="static/images/keypoint_transfer.png" alt="Overview of Proposed Method">
          </figure>
          <figcaption class="has-text-centered">Correspondences (%) of keypoint transfer on KeypointNet</figcaption>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/supp_qual_part_motorcycle.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Motorcycle
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/supp_qual_part_airplane.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Airplane
        </h2>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/vOjomwoc-JY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/poster_compressed.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{park2024learning,
          title={Learning SO(3)-Invariant Semantic Correspondence via Local Shape Transform},
          author={Park, Chunghyun and Kim, Seungwook and Park, Jaesik and Cho, Minsu},
          booktitle={Proceedings of the {IEEE/CVF} Conference on Computer Vision and Pattern Recognition (CVPR)},
          month={June},
          year={2024},
         }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
